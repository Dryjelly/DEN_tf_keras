{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 1907810684105767984,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 16820970885105611435\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 1104621815741727784\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10486821696\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 3261008697050369161\n",
       " physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import copy \n",
    "from Dataset import *\n",
    "from collections import defaultdict\n",
    "from numpy import linalg as LA\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_lambda = 0.00001  # Sparsity for L1\n",
    "l2_lambda = 0.0001   # L2 lambda\n",
    "gl_lambda = 0.001    # Group Lasso lambda\n",
    "regular_lambda = 15.0 # 0.5 # regularization lambda\n",
    "\n",
    "ex_k = 10       # The number of units increased in the expansion processing\n",
    "loss_thr = 0.01 # Threshold of dynamic expansion\n",
    "spl_thr = 0.05  # Threshold of split and duplication\n",
    "\n",
    "class_num = 2\n",
    "\n",
    "GL_var = [] # [var for var in tf.trainable_variables() if 'new' in var.name and ('bw' in var.name or 'tw' in var.name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        \n",
    "        # Cut-off ---------------------------------------------------------------------------------------------------------\n",
    "        for var in trainable_vars:\n",
    "            th_t = tf.fill(tf.shape(var), l1_lambda)\n",
    "            zero_t = tf.zeros(tf.shape(var))\n",
    "            var_temp = var - (th_t * tf.sign(var))\n",
    "            var.assign(tf.where(tf.less(tf.abs(var), th_t), zero_t, var_temp))\n",
    "\n",
    "        # Gropu Cut-off\n",
    "        for var in GL_var:\n",
    "            #print(\"in the group cut off part!\")\n",
    "            g_sum = tf.sqrt(tf.reduce_sum(tf.square(var), 0))\n",
    "            th_t = gl_lambda\n",
    "            gw = []\n",
    "            for i in range(var.get_shape()[1]):\n",
    "                temp_gw = var[:, i] - (th_t * var[:, i] / g_sum[i])\n",
    "                gw_gl = tf.where(tf.less(g_sum[i], th_t), tf.zeros(tf.shape(var[:, i])), temp_gw)\n",
    "                gw.append(gw_gl)\n",
    "            var.assign(tf.stack(gw, 1))\n",
    "        # -----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "def _regular_lambda_loss(y_true, y_pred, lam, prev_w, cur_w):\n",
    "    regular_terms = []\n",
    "    for prev_var, cur_var in zip(prev_w, cur_w):\n",
    "        regular_terms.append(tf.nn.l2_loss(cur_var-prev_var))\n",
    "    loss = tf.reduce_sum(lam * tf.reduce_sum(regular_terms) + tf.reduce_sum(tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)))\n",
    "    return loss\n",
    "\n",
    "def regular_lambda_loss(lam, prev_w, cur_w):\n",
    "    def _temp_loss(y_true, y_pred):\n",
    "        return _regular_lambda_loss(y_true, y_pred, lam, prev_w, cur_w)\n",
    "    return _temp_loss\n",
    "\n",
    "reg_l2 = tf.keras.regularizers.l2(l=l2_lambda)\n",
    "\n",
    "def zero_node_count(model):\n",
    "    for layer in model.layers:\n",
    "        if layer.trainable == False: continue\n",
    "        w = layer.get_weights()\n",
    "        for _w in w:\n",
    "            print(f\"layer \\\"{layer.name}\\\" weight shape : {_w.shape}\")\n",
    "            if len(_w.shape) > 1:\n",
    "                cnt = 0\n",
    "                for j in range(_w.shape[0]):\n",
    "                    if np.count_nonzero(_w[j, :]) == 0:\n",
    "                        #print(f'{j} node is zero')\n",
    "                        cnt += 1\n",
    "                print(f\"\\nnumber of zero node : {cnt}\")\n",
    "            print(f\"number of zero weight : {np.sum(_w==0)}\")\n",
    "            print(f\"min value of weight : {np.min(np.abs(_w))}\\n\")\n",
    "\n",
    "def new_task_model(tree_model, Task_number):\n",
    "    inputs = tree_model.get_layer(\"input_layer\").input\n",
    "    h = tree_model.layers[-2].output\n",
    "    outputs = tf.keras.layers.Dense(class_num , activation='softmax', name = f\"T{Task_number}_output\")(h)\n",
    "\n",
    "    return CustomModel(inputs, outputs)\n",
    "\n",
    "def selective_learning(model, dataset):\n",
    "    selected_prev_params = dict()   \n",
    "    selected_params = dict()\n",
    "    all_indices = defaultdict(list) # nonzero unis \n",
    "\n",
    "    selected_model = model_t2\n",
    "    n_layer = len(model_t2.layers)\n",
    "\n",
    "    # 상위 레이어 부터 골라내기\n",
    "    for i in range(n_layer-1,0,-1):\n",
    "        w = selected_model.layers[i].get_weights()[0]\n",
    "        b = selected_model.layers[i].get_weights()[1]\n",
    "        if i == n_layer-1:\n",
    "            for j in range(w.shape[0]):\n",
    "                if np.count_nonzero(w[j, :])!= 0:\n",
    "                    all_indices['layer%d'%i].append(j)\n",
    "            selected_params['layer%d/weight'%(i)] = w[np.ix_(all_indices['layer%d'%i], list(range(class_num)))]\n",
    "            selected_params['layer%d/biases'%(i)] = b\n",
    "        else:\n",
    "            top_indices = all_indices['layer%d'%(i+1)]\n",
    "            for j in range(w.shape[0]):\n",
    "                if np.count_nonzero(w[j, top_indices])!= 0 or i == 1: # input과 연결된 node는 전부 쓰기 위한 i == 1\n",
    "                    all_indices['layer%d'%i].append(j)\n",
    "\n",
    "            sub_weight = w[np.ix_(all_indices['layer%d'%i], top_indices)]\n",
    "            sub_biases = b[all_indices['layer%d'%(i+1)]]\n",
    "            selected_params['layer%d/weight'%i] = sub_weight\n",
    "            selected_params['layer%d/biases'%i] = sub_biases\n",
    "            selected_prev_params['layer%d/weight'%i] = sub_weight\n",
    "            selected_prev_params['layer%d/biases'%i] = sub_biases\n",
    "\n",
    "    print(f\"seleted_params ------\\n\")\n",
    "    for i in selected_params.items():\n",
    "        print(i[0], i[1].shape)\n",
    "\n",
    "    selected_w = []\n",
    "    for i in range(1,4):\n",
    "        if i == 1 : h = inputs = tf.keras.Input(shape=(model.input_shape[-1],))\n",
    "        else : h = tf.keras.layers.Dense(selected_params[f\"layer{i}/weight\"].shape[0], activation='relu', kernel_regularizer=reg_l2, bias_regularizer=reg_l2)(h)\n",
    "        selected_w.append(selected_params[f\"layer{i}/weight\"])\n",
    "        selected_w.append(selected_params[f\"layer{i}/biases\"])\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(class_num , activation='softmax', kernel_regularizer=reg_l2, bias_regularizer=reg_l2)(h)\n",
    "    model_select = CustomModel(inputs, outputs)\n",
    "    model_select.set_weights(selected_w)\n",
    "\n",
    "    cur_w = model_select.trainable_variables\n",
    "    prev_w = list(map(lambda x:x.numpy(), model_select.trainable_variables))\n",
    "    model_select.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=regular_lambda_loss(regular_lambda, prev_w, cur_w), metrics=['accuracy'])\n",
    "    model_select.summary()\n",
    "\n",
    "    images_train, labels_train, images_test, labels_test = dataset\n",
    "    history = model_select.fit(images_train, labels_train, validation_data=(images_test, labels_test), epochs=10, batch_size=100, verbose=1)\n",
    "\n",
    "    # union\n",
    "    selected_w = model_select.get_weights()\n",
    "\n",
    "    n_layer = len(model.layers)\n",
    "\n",
    "    # seleted train weights 를 기존 모델에 결합 (상위 레이어부터 차례로 결합)\n",
    "    for i in range(n_layer-1,0,-1):\n",
    "        w = model.layers[i].get_weights()[0]\n",
    "        b = model.layers[i].get_weights()[1]\n",
    "        if i == n_layer-1:\n",
    "            temp_weight = w\n",
    "            temp_weight[np.ix_(all_indices['layer%d'%i], list(range(class_num)))] = selected_w[i*2-2]\n",
    "            model.layers[i].set_weights([temp_weight, selected_w[i*2-1]])\n",
    "        else:\n",
    "            temp_weight = w\n",
    "            temp_biases = b\n",
    "            temp_weight[np.ix_(all_indices['layer%d'%i], all_indices['layer%d'%(i+1)])] = selected_w[i*2-2]\n",
    "            temp_biases[all_indices['layer%d'%(i+1)]] = selected_w[i*2-1]\n",
    "            model.layers[i].set_weights([temp_weight, temp_biases])\n",
    "\n",
    "def network_expansion(model, ex_k, dataset):\n",
    "    inputs = model.get_layer(\"input_layer\").input\n",
    "\n",
    "    for i, layer in enumerate(model.layers[1:-1]):\n",
    "        if i == 0:\n",
    "            new_h = tf.keras.layers.Dense(ex_k, activation='relu', kernel_regularizer=reg_l2, bias_regularizer=reg_l2, name = f\"layer{i+1}_tw\")(inputs)\n",
    "            expanded_h = tf.keras.layers.Concatenate(name = f\"layer{i+1}_tw_concat\")([layer.output, new_h])\n",
    "            new_output = layer.output\n",
    "        else:\n",
    "            new_h_add = tf.keras.layers.Dense(layer.output_shape[-1], activation='relu', kernel_regularizer=reg_l2, use_bias=False, name = f\"layer{i+1}_bw\")(new_h)\n",
    "            new_output = tf.keras.layers.Add(name = f\"layer{i+1}_bw_add\")([new_h_add, layer(prev_output)])\n",
    "            new_h = tf.keras.layers.Dense(ex_k, activation='relu', kernel_regularizer=reg_l2, bias_regularizer=reg_l2, name = f\"layer{i+1}_tw\")(expanded_h)\n",
    "            expanded_h = tf.keras.layers.Concatenate(name = f\"layer{i+1}_tw_concat\")([new_output, new_h])\n",
    "        prev_output = new_output\n",
    "    \n",
    "    new_h_add = tf.keras.layers.Dense(model.layers[-1].output_shape[-1], activation='relu', kernel_regularizer=reg_l2, use_bias=False, name = f\"layer{len(model.layers)-1}_bw\")(new_h)\n",
    "    outputs = tf.keras.layers.Add(name = f\"layer{len(model.layers)-1}_add\")([new_h_add, model.layers[-1](new_output)])\n",
    "\n",
    "    model_expansion = CustomModel(inputs, outputs)\n",
    "\n",
    "    # cur_w = network_2.trainable_variables\n",
    "    # prev_w = list(map(lambda x:x.numpy(), network_2.trainable_variables))\n",
    "    model_expansion.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])#, run_eagerly=True)\n",
    "    model_expansion.summary()\n",
    "    #GL_var = [var for var in model_t3.trainable_variables if len(var.get_shape()) > 1] # [var for var in tf.trainable_variables() if 'new' in var.name and ('bw' in var.name or 'tw' in var.name)]\n",
    "\n",
    "    images_train, labels_train, images_test, labels_test = dataset\n",
    "    #(images_train, labels_train),(images_test, labels_test) = specific_load('emnist', add_channel = False, emnist_type = 'upper', choose = list(range(10)))\n",
    "    history = model_expansion.fit(images_train, labels_train, validation_data=(images_test, labels_test), epochs=6, batch_size=100, verbose=1)\n",
    "\n",
    "    # --------------------------------------------- 골라내기\n",
    "    extended_params = dict()\n",
    "\n",
    "    n_layers = len(model.layers)\n",
    "\n",
    "    for i in range(n_layers-2, 0, -1): # 2~1\n",
    "\n",
    "        bw_layer = model_expansion.get_layer(f\"layer{i+1}_bw\")\n",
    "        tw_layer = model_expansion.get_layer(f\"layer{i}_tw\")\n",
    "\n",
    "        print(bw_layer.get_weights()[0].shape, tw_layer.get_weights()[0].shape)\n",
    "\n",
    "        if i != n_layers-2 : prev_tw_w = extended_params[f\"layer{i+1}_tw\"][0]\n",
    "        bw_w = bw_layer.get_weights()[0]\n",
    "        tw_w = tw_layer.get_weights()[0]\n",
    "        tw_b = tw_layer.get_weights()[1]\n",
    "        \n",
    "\n",
    "        useless = []\n",
    "        for j in range(tw_w.shape[1]):\n",
    "            if np.count_nonzero(tw_w[:, j]) == 0:\n",
    "                useless.append(j)\n",
    "\n",
    "        print(f\"   [*] Expanding {i}th hidden unit, {ex_k - len(useless)} unit added\")\n",
    "        extended_params[bw_layer.name] = [np.delete(bw_w, useless, axis = 0)]\n",
    "        extended_params[tw_layer.name] = [np.delete(tw_w, useless, axis = 1), np.delete(tw_b, useless)]\n",
    "        if i != n_layers-2 and len(useless) != 0:\n",
    "            extended_params[f\"layer{i+1}_tw\"][0] = np.delete(prev_tw_w, np.array(useless) + prev_tw_w.shape[0] - ex_k, axis = 0)\n",
    "            \n",
    "    # --------------------------------------- cur_w\n",
    "    model_cur_w = []\n",
    "\n",
    "    for l in model.layers[1:]:\n",
    "        model_cur_w.append(l.get_weights()[0])\n",
    "        model_cur_w.append(l.get_weights()[1])\n",
    "\n",
    "    for weight in model_cur_w:\n",
    "        print(weight.shape)\n",
    "    # --------------------------------------- new_w\n",
    "    model_new_w = []\n",
    "\n",
    "    for i, value in enumerate(model_cur_w):\n",
    "        temp_w = value\n",
    "        idx = i//2\n",
    "        if len(value.shape) > 1:\n",
    "            if f'layer{idx+1}_bw' in extended_params:\n",
    "                extend_weight = extended_params[f'layer{idx+1}_bw'][0]\n",
    "                temp_w = np.concatenate((temp_w ,extend_weight), axis = 0)\n",
    "            if f'layer{idx+1}_tw' in extended_params:\n",
    "                extend_weight = extended_params[f'layer{idx+1}_tw'][0]\n",
    "                temp_w = np.concatenate((temp_w ,extend_weight), axis = 1)\n",
    "        else:\n",
    "            if f'layer{idx+1}_tw' in extended_params:\n",
    "                extend_weight = extended_params[f'layer{idx+1}_tw'][1]\n",
    "                temp_w = np.concatenate((temp_w ,extend_weight))\n",
    "        model_new_w.append(temp_w)\n",
    "        print(temp_w.shape)\n",
    "\n",
    "    return model_new_w\n",
    "\n",
    "def split_expansion(model, model_prev_w, model_new_w, dataset):\n",
    "    # find the highly drifted ones and split\n",
    "    unit_indices = []\n",
    "    for prev, cur in zip(model_prev_w[:-2], model_new_w[:-2]):\n",
    "        if len(prev.shape) == 1 : continue # bias 는 거름\n",
    "        next_dim = prev.shape[1]\n",
    "\n",
    "        indices = []\n",
    "        cosims = []\n",
    "        for j in range(next_dim):\n",
    "            cosim = LA.norm(prev[:, j] - cur[:prev.shape[0], j])\n",
    "            #print(cosim, prev[:, j], cur[:prev.shape[0], j])\n",
    "            if cosim > spl_thr: # spl_thr = 0.05\n",
    "                indices.append(j)\n",
    "                cosims.append(cosim)\n",
    "        _temp = np.argsort(cosims)[:ex_k] # ex_k = 10 , 10개까지 추가 제한\n",
    "        print(\"   [*] split N in layer: %d / %d\"%(len(_temp), len(cosims)))\n",
    "        indices = np.array(indices)[_temp]\n",
    "        unit_indices.append(indices)\n",
    "\n",
    "    prev_W_split = copy.deepcopy(model_new_w)\n",
    "    for i, w in enumerate(model_prev_w[:-2]):\n",
    "        temp = prev_W_split[i]\n",
    "        if len(w.shape) >= 2:\n",
    "            temp[:w.shape[0], :w.shape[1]] = w\n",
    "        else:\n",
    "            temp[:w.shape[0]] = w\n",
    "        prev_W_split[i] = temp\n",
    "\n",
    "    # ------------------------------------ \n",
    "    final_weight = []\n",
    "\n",
    "    for i in range(len(unit_indices)):\n",
    "        prev_w = np.copy(prev_W_split[i*2])\n",
    "        cur_w = np.copy(model_new_w[i*2])\n",
    "        indices = unit_indices[i]\n",
    "        next_dim = prev_w.shape[1]\n",
    "        if i >= 1:\n",
    "            below_dim = prev_w.shape[0]\n",
    "            below_indices = unit_indices[i-1]\n",
    "            bottom_p_prev_ary, bottom_p_new_ary, bottom_c_prev_ary, bottom_c_new_ary = [], [], [], []\n",
    "            for j in range(below_dim):\n",
    "                if j in below_indices:\n",
    "                    bottom_p_prev_ary.append(prev_w[j, :])\n",
    "                    bottom_p_new_ary.append(cur_w[j, :])\n",
    "                    bottom_c_prev_ary.append(cur_w[j, :])\n",
    "                    bottom_c_new_ary.append(cur_w[j, :])\n",
    "                else:\n",
    "                    bottom_p_prev_ary.append(cur_w[j, :])\n",
    "                    bottom_c_prev_ary.append(cur_w[j, :])\n",
    "            prev_w = np.array( bottom_p_prev_ary + bottom_p_new_ary ).astype(np.float32)\n",
    "            cur_w = np.array( bottom_c_prev_ary + bottom_c_new_ary ).astype(np.float32)\n",
    "\n",
    "        prev_ary = []\n",
    "        new_ary = []\n",
    "        for j in range(next_dim):\n",
    "            if j in indices:\n",
    "                prev_ary.append(prev_w[:, j]) \n",
    "                new_ary.append(cur_w[:, j]) # will be expanded\n",
    "            else:\n",
    "                prev_ary.append(cur_w[:, j])\n",
    "        # fully connected, L1\n",
    "        expanded_w = np.array( prev_ary + new_ary ).T.astype(np.float32)\n",
    "        expanded_b = np.concatenate((prev_W_split[i*2+1], np.random.rand(len(new_ary)))).astype(np.float32)\n",
    "\n",
    "        final_weight.append([expanded_w, expanded_b])\n",
    "        print(f\"expanded_w shape = {expanded_w.shape}\")\n",
    "        print(f\"expanded_b shape = {expanded_b.shape}\")\n",
    "    # ----------------------------------------\n",
    "\n",
    "    node_number_expansion = [n.shape[0]-p.shape[0] for p, n in zip(model_prev_w[1:-2:2], model_new_w[1:-2:2])]\n",
    "    node_number_split = [len(i) for i in unit_indices]\n",
    "    node_number_add = [e+s for e, s in zip(node_number_expansion, node_number_split)]\n",
    "    print(node_number_expansion, node_number_split, node_number_add)\n",
    "\n",
    "    inputs = model.get_layer(\"input_layer\").input\n",
    "\n",
    "    for i, layer in enumerate(model.layers[1:-1]):\n",
    "        if i == 0:\n",
    "            new_h = tf.keras.layers.Dense(node_number_add[i], activation='relu', kernel_regularizer=reg_l2, bias_regularizer=reg_l2, name = f\"layer{i+1}_tw_select\")(inputs)\n",
    "\n",
    "            expanded_h = tf.keras.layers.Concatenate(name = f\"layer{i+1}_tw_concat_select\")([layer.output, new_h])\n",
    "            new_output = layer.output\n",
    "        else:\n",
    "            new_h_add = tf.keras.layers.Dense(layer.output_shape[-1], activation='relu', kernel_regularizer=reg_l2, use_bias=False, name = f\"layer{i+1}_bw_select\")(new_h)\n",
    "\n",
    "            new_output = tf.keras.layers.Add(name = f\"layer{i+1}_bw_add_select\")([new_h_add, layer(prev_output)])\n",
    "            new_h = tf.keras.layers.Dense(node_number_add[i], activation='relu', kernel_regularizer=reg_l2, bias_regularizer=reg_l2, name = f\"layer{i+1}_tw_select\")(expanded_h)\n",
    "\n",
    "            expanded_h = tf.keras.layers.Concatenate(name = f\"layer{i+1}_tw_concat_select\")([new_output, new_h])\n",
    "        prev_output = new_output\n",
    "\n",
    "    new_h_add = tf.keras.layers.Dense(model.layers[-1].output_shape[-1], activation='relu', kernel_regularizer=reg_l2, use_bias=False, name = f\"layer{len(model.layers)-1}_bw_select\")(new_h)\n",
    "\n",
    "    outputs = tf.keras.layers.Add(name = f\"layer{len(model.layers)-1}_add_select\")([new_h_add, model.layers[-1](new_output)])\n",
    "\n",
    "    model_final = CustomModel(inputs, outputs)\n",
    "\n",
    "    model_final.get_layer(\"layer1_tw_select\").set_weights([final_weight[0][0][:,-node_number_add[0]:], final_weight[0][1][-node_number_add[0]:]])\n",
    "    model_final.get_layer(\"layer2_bw_select\").set_weights([final_weight[1][0][-node_number_add[0]:,:model_prev_w[3].shape[0]]])\n",
    "    model_final.get_layer(\"layer2_tw_select\").set_weights([final_weight[1][0][:,-node_number_add[1]:], final_weight[1][1][-node_number_add[1]:]])\n",
    "    temp = model_final.get_layer(\"layer3_bw_select\").get_weights()\n",
    "    temp[0][:node_number_expansion[-1],:] = model_new_w[-2][-node_number_expansion[-1]:,:]\n",
    "\n",
    "    model_final.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])#, run_eagerly=True)\n",
    "    model_final.summary()\n",
    "\n",
    "    images_train, labels_train, images_test, labels_test = dataset\n",
    "    history = model_final.fit(images_train, labels_train, validation_data=(images_test, labels_test), epochs=6, batch_size=100, verbose=1)\n",
    "    # --------------------------------------------- 골라내기\n",
    "    extended_params = dict()\n",
    "\n",
    "    n_layers = len(model.layers)\n",
    "\n",
    "    for i in range(n_layers-2, 0, -1): # 2~1\n",
    "\n",
    "        bw_layer = model_final.get_layer(f\"layer{i+1}_bw_select\")\n",
    "        tw_layer = model_final.get_layer(f\"layer{i}_tw_select\")\n",
    "\n",
    "        print(bw_layer.get_weights()[0].shape, tw_layer.get_weights()[0].shape)\n",
    "\n",
    "        bw_w = bw_layer.get_weights()[0]\n",
    "        tw_w = tw_layer.get_weights()[0]\n",
    "        tw_b = tw_layer.get_weights()[1]\n",
    "\n",
    "        extended_params[bw_layer.name] = [bw_w]\n",
    "        extended_params[tw_layer.name] = [tw_w, tw_b]\n",
    "\n",
    "    # --------------------------------------- cur_w\n",
    "    model_cur_w = []\n",
    "\n",
    "    for l in model.layers[1:]:\n",
    "        model_cur_w.append(l.get_weights()[0])\n",
    "        model_cur_w.append(l.get_weights()[1])\n",
    "\n",
    "    for weight in model_cur_w:\n",
    "        print(weight.shape)\n",
    "    # --------------------------------------- new_w\n",
    "    model_new_w = []\n",
    "\n",
    "    for i, value in enumerate(model_cur_w):\n",
    "        temp_w = value\n",
    "        idx = i//2\n",
    "        if len(value.shape) > 1:\n",
    "            if f'layer{idx+1}_bw_select' in extended_params:\n",
    "                extend_weight = extended_params[f'layer{idx+1}_bw_select'][0]\n",
    "                temp_w = np.concatenate((temp_w ,extend_weight), axis = 0)\n",
    "            if f'layer{idx+1}_tw_select' in extended_params:\n",
    "                extend_weight = extended_params[f'layer{idx+1}_tw_select'][0]\n",
    "                temp_w = np.concatenate((temp_w ,extend_weight), axis = 1)\n",
    "        else:\n",
    "            if f'layer{idx+1}_tw_select' in extended_params:\n",
    "                extend_weight = extended_params[f'layer{idx+1}_tw_select'][1]\n",
    "                temp_w = np.concatenate((temp_w ,extend_weight))\n",
    "        model_new_w.append(temp_w)\n",
    "        print(temp_w.shape)\n",
    "\n",
    "    return model_new_w\n",
    "\n",
    "def make_models_by_weight(weight, Task_num, prev_models):\n",
    "    models = []\n",
    "    inputs = h = tf.keras.Input(shape=(weight[0].shape[0],), name = \"input_layer\")\n",
    "\n",
    "    for w in weight[:-2:2]:\n",
    "        h = tf.keras.layers.Dense(w.shape[1], activation='relu', kernel_regularizer=reg_l2, bias_regularizer=reg_l2)(h)\n",
    "    \n",
    "    for T in range(1, Task_num + 1):\n",
    "        outputs = tf.keras.layers.Dense(class_num , activation='softmax', kernel_regularizer=reg_l2, bias_regularizer=reg_l2, name = f'T{T}_output')(h)\n",
    "        new_model = CustomModel(inputs, outputs)\n",
    "        new_model.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        models.append(new_model)\n",
    "\n",
    "    models[-1].set_weights(weight)\n",
    "\n",
    "    for m, p_m in zip(models[:-1], prev_models):\n",
    "        temp = p_m.layers[-1].get_weights()[0]\n",
    "        temp_shape = temp.shape[0]\n",
    "        extend_shape = weight[-2].shape[0]\n",
    "        \n",
    "        new_temp = np.concatenate((temp, np.zeros((extend_shape-temp_shape, class_num))), axis = 0)\n",
    "\n",
    "        new_temp_weight = [new_temp, copy.deepcopy(p_m.layers[-1].get_weights()[1])]\n",
    "        m.layers[-1].set_weights(new_temp_weight)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 312)               244920    \n",
      "_________________________________________________________________\n",
      "T1_feature (Dense)           (None, 128)               40064     \n",
      "_________________________________________________________________\n",
      "T1_output (Dense)            (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 285,242\n",
      "Trainable params: 285,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(shape=(784,), name = \"input_layer\")\n",
    "h = tf.keras.layers.Dense(312, activation='relu', kernel_regularizer=reg_l2, bias_regularizer=reg_l2)(inputs)\n",
    "h = tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=reg_l2, bias_regularizer=reg_l2, name = 'T1_feature')(h)\n",
    "outputs = tf.keras.layers.Dense(class_num , activation='softmax', kernel_regularizer=reg_l2, bias_regularizer=reg_l2, name = 'T1_output')(h)\n",
    "\n",
    "model_t1 = CustomModel(inputs, outputs)\n",
    "model_t1.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])#, run_eagerly=True)\n",
    "model_t1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "113/113 [==============================] - 0s 3ms/step - loss: 0.0973 - accuracy: 0.9824 - val_loss: 0.0488 - val_accuracy: 0.9968\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.0442 - accuracy: 0.9964 - val_loss: 0.0408 - val_accuracy: 0.9979\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0329 - accuracy: 0.9992 - val_loss: 0.0300 - val_accuracy: 0.9995\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 0.9998 - val_loss: 0.0259 - val_accuracy: 0.9984\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 0.9996 - val_loss: 0.0231 - val_accuracy: 0.9989\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 0.9995\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 0s 1ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 0.9995\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 0.9979\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 0.9984\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9999 - val_loss: 0.0114 - val_accuracy: 0.9984\n"
     ]
    }
   ],
   "source": [
    "#(images_train, labels_train),(images_test, labels_test) = load_mnist()\n",
    "(images_train, labels_train),(images_test, labels_test) = specific_load('mnist', add_channel = False, choose = [4,5])\n",
    "_ = model_t1.fit(images_train, labels_train, validation_data=(images_test, labels_test), epochs=10, batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer \"dense\" weight shape : (784, 312)\n",
      "\n",
      "number of zero node : 218\n",
      "number of zero weight : 100791\n",
      "min value of weight : 0.0\n",
      "\n",
      "layer \"dense\" weight shape : (312,)\n",
      "number of zero weight : 21\n",
      "min value of weight : 0.0\n",
      "\n",
      "layer \"T1_feature\" weight shape : (312, 128)\n",
      "\n",
      "number of zero node : 1\n",
      "number of zero weight : 6695\n",
      "min value of weight : 0.0\n",
      "\n",
      "layer \"T1_feature\" weight shape : (128,)\n",
      "number of zero weight : 18\n",
      "min value of weight : 0.0\n",
      "\n",
      "layer \"T1_output\" weight shape : (128, 2)\n",
      "\n",
      "number of zero node : 6\n",
      "number of zero weight : 12\n",
      "min value of weight : 0.0\n",
      "\n",
      "layer \"T1_output\" weight shape : (2,)\n",
      "number of zero weight : 0\n",
      "min value of weight : 0.0005245982902124524\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zero_node_count(model_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 312)               244920    \n",
      "_________________________________________________________________\n",
      "T1_feature (Dense)           (None, 128)               40064     \n",
      "_________________________________________________________________\n",
      "T2_output (Dense)            (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 285,242\n",
      "Trainable params: 258\n",
      "Non-trainable params: 284,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_t1_prev_w = copy.deepcopy(model_t1.get_weights())\n",
    "model_t1.trainable = False\n",
    "\n",
    "# cur_w = network_2.trainable_variables\n",
    "# prev_w = list(map(lambda x:x.numpy(), network_2.trainable_variables))\n",
    "model_t2 = new_task_model(tree_model = model_t1, Task_number = 2)\n",
    "model_t2.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_t2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 0.3755 - accuracy: 0.8493 - val_loss: 0.2780 - val_accuracy: 0.9012\n",
      "Epoch 2/10\n",
      "127/127 [==============================] - 0s 1ms/step - loss: 0.2592 - accuracy: 0.9076 - val_loss: 0.2079 - val_accuracy: 0.9437\n",
      "Epoch 3/10\n",
      "127/127 [==============================] - 0s 1ms/step - loss: 0.1992 - accuracy: 0.9407 - val_loss: 0.1599 - val_accuracy: 0.9622\n",
      "Epoch 4/10\n",
      "127/127 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9600 - val_loss: 0.1285 - val_accuracy: 0.9745\n",
      "Epoch 5/10\n",
      "127/127 [==============================] - 0s 1ms/step - loss: 0.1302 - accuracy: 0.9711 - val_loss: 0.1079 - val_accuracy: 0.9792\n",
      "Epoch 6/10\n",
      "127/127 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9780 - val_loss: 0.0912 - val_accuracy: 0.9839\n",
      "Epoch 7/10\n",
      "127/127 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.9819 - val_loss: 0.0797 - val_accuracy: 0.9877\n",
      "Epoch 8/10\n",
      "127/127 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.9844 - val_loss: 0.0709 - val_accuracy: 0.9901\n",
      "Epoch 9/10\n",
      "127/127 [==============================] - 0s 1ms/step - loss: 0.0766 - accuracy: 0.9858 - val_loss: 0.0640 - val_accuracy: 0.9920\n",
      "Epoch 10/10\n",
      "127/127 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.9871 - val_loss: 0.0590 - val_accuracy: 0.9929\n"
     ]
    }
   ],
   "source": [
    "#(images_train, labels_train),(images_test, labels_test) = load_fashion_mnist()\n",
    "(images_train, labels_train),(images_test, labels_test) = specific_load('mnist', add_channel = False, choose = [0,1])\n",
    "_ = model_t2.fit(images_train, labels_train, validation_data=(images_test, labels_test), epochs=10, batch_size=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer \"T2_output\" weight shape : (128, 2)\n",
      "\n",
      "number of zero node : 0\n",
      "number of zero weight : 2\n",
      "min value of weight : 0.0\n",
      "\n",
      "layer \"T2_output\" weight shape : (2,)\n",
      "number of zero weight : 0\n",
      "min value of weight : 0.5772326588630676\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zero_node_count(model_t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seleted_params ------\n",
      "\n",
      "layer3/weight (128, 2)\n",
      "layer3/biases (2,)\n",
      "layer2/weight (311, 128)\n",
      "layer2/biases (128,)\n",
      "layer1/weight (784, 311)\n",
      "layer1/biases (311,)\n",
      "Model: \"custom_model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 311)               244135    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               39936     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 284,329\n",
      "Trainable params: 284,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 2.8256 - accuracy: 0.9969 - val_loss: 2.2777 - val_accuracy: 0.9981\n",
      "Epoch 2/10\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 2.5287 - accuracy: 0.9968 - val_loss: 2.2372 - val_accuracy: 0.9991\n",
      "Epoch 3/10\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 3.1194 - accuracy: 0.9968 - val_loss: 2.4481 - val_accuracy: 0.9991\n",
      "Epoch 4/10\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 2.3789 - accuracy: 0.9973 - val_loss: 2.6733 - val_accuracy: 0.9981\n",
      "Epoch 5/10\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 2.7693 - accuracy: 0.9966 - val_loss: 1.9167 - val_accuracy: 0.9995\n",
      "Epoch 6/10\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 2.4968 - accuracy: 0.9975 - val_loss: 2.9045 - val_accuracy: 0.9976\n",
      "Epoch 7/10\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 2.9944 - accuracy: 0.9967 - val_loss: 2.2076 - val_accuracy: 0.9976\n",
      "Epoch 8/10\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 2.6699 - accuracy: 0.9968 - val_loss: 2.9685 - val_accuracy: 0.9981\n",
      "Epoch 9/10\n",
      "127/127 [==============================] - 0s 3ms/step - loss: 2.5305 - accuracy: 0.9974 - val_loss: 2.2494 - val_accuracy: 0.9991\n",
      "Epoch 10/10\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 2.8994 - accuracy: 0.9965 - val_loss: 3.1450 - val_accuracy: 0.9995\n"
     ]
    }
   ],
   "source": [
    "selective_learning(model_t2, (images_train, labels_train, images_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 0s 794us/step - loss: 0.0126 - accuracy: 0.9995\n",
      "59/59 [==============================] - 0s 914us/step - loss: 0.0141 - accuracy: 0.9989\n"
     ]
    }
   ],
   "source": [
    "history = model_t2.evaluate(images_test,labels_test)\n",
    "\n",
    "#(images_train, labels_train),(images_test, labels_test) = load_mnist()\n",
    "(images_train, labels_train),(images_test, labels_test) = specific_load('mnist', add_channel = False, choose = [4,5])\n",
    "_ = model_t1.evaluate(images_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012575163505971432 > 0.01\n",
      "Model: \"custom_model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer (InputLayer)        [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 312)          244920      input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer1_tw (Dense)               (None, 10)           7850        input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer1_tw_concat (Concatenate)  (None, 322)          0           dense[0][0]                      \n",
      "                                                                 layer1_tw[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer2_bw (Dense)               (None, 128)          1280        layer1_tw[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "T1_feature (Dense)              (None, 128)          40064       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer2_tw (Dense)               (None, 10)           3230        layer1_tw_concat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer2_bw_add (Add)             (None, 128)          0           layer2_bw[0][0]                  \n",
      "                                                                 T1_feature[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer3_bw (Dense)               (None, 2)            20          layer2_tw[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "T2_output (Dense)               (None, 2)            258         layer2_bw_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer3_add (Add)                (None, 2)            0           layer3_bw[0][0]                  \n",
      "                                                                 T2_output[1][0]                  \n",
      "==================================================================================================\n",
      "Total params: 297,622\n",
      "Trainable params: 12,638\n",
      "Non-trainable params: 284,984\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/6\n",
      "127/127 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.9991 - val_loss: 0.0134 - val_accuracy: 0.9995\n",
      "Epoch 2/6\n",
      "127/127 [==============================] - 0s 4ms/step - loss: 0.0136 - accuracy: 0.9991 - val_loss: 0.0124 - val_accuracy: 0.9995\n",
      "Epoch 3/6\n",
      "127/127 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9995 - val_loss: 0.0115 - val_accuracy: 0.9991\n",
      "Epoch 4/6\n",
      "127/127 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 0.9995 - val_loss: 0.0108 - val_accuracy: 0.9995\n",
      "Epoch 5/6\n",
      "127/127 [==============================] - 0s 4ms/step - loss: 0.0105 - accuracy: 0.9996 - val_loss: 0.0106 - val_accuracy: 0.9995\n",
      "Epoch 6/6\n",
      "127/127 [==============================] - 0s 3ms/step - loss: 0.0101 - accuracy: 0.9998 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "(10, 2) (322, 10)\n",
      "   [*] Expanding 2th hidden unit, 10 unit added\n",
      "(10, 128) (784, 10)\n",
      "   [*] Expanding 1th hidden unit, 10 unit added\n",
      "(784, 312)\n",
      "(312,)\n",
      "(312, 128)\n",
      "(128,)\n",
      "(128, 2)\n",
      "(2,)\n",
      "(784, 322)\n",
      "(322,)\n",
      "(322, 138)\n",
      "(138,)\n",
      "(138, 2)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "if history[0] > loss_thr:\n",
    "    ### distory_graph 나중에 추가할 것\n",
    "    print(f\"{history[0]} > {loss_thr}\")\n",
    "    #(images_train, labels_train),(images_test, labels_test) = load_fashion_mnist()\n",
    "    (images_train, labels_train),(images_test, labels_test) = specific_load('mnist', add_channel = False, choose = [0,1])\n",
    "    model_new_w = network_expansion(model_t2, ex_k, (images_train, labels_train, images_test, labels_test))\n",
    "else:\n",
    "    model_new_w = model_t2.get_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [*] split N in layer: 10 / 25\n",
      "   [*] split N in layer: 2 / 2\n",
      "expanded_w shape = (784, 332)\n",
      "expanded_b shape = (332,)\n",
      "expanded_w shape = (332, 140)\n",
      "expanded_b shape = (140,)\n",
      "[10, 10] [10, 2] [20, 12]\n",
      "Model: \"custom_model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer (InputLayer)        [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 312)          244920      input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer1_tw_select (Dense)        (None, 20)           15700       input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer1_tw_concat_select (Concat (None, 332)          0           dense[0][0]                      \n",
      "                                                                 layer1_tw_select[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer2_bw_select (Dense)        (None, 128)          2560        layer1_tw_select[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "T1_feature (Dense)              (None, 128)          40064       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer2_tw_select (Dense)        (None, 12)           3996        layer1_tw_concat_select[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer2_bw_add_select (Add)      (None, 128)          0           layer2_bw_select[0][0]           \n",
      "                                                                 T1_feature[2][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer3_bw_select (Dense)        (None, 2)            24          layer2_tw_select[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "T2_output (Dense)               (None, 2)            258         layer2_bw_add_select[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer3_add_select (Add)         (None, 2)            0           layer3_bw_select[0][0]           \n",
      "                                                                 T2_output[2][0]                  \n",
      "==================================================================================================\n",
      "Total params: 307,522\n",
      "Trainable params: 22,538\n",
      "Non-trainable params: 284,984\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/6\n",
      "127/127 [==============================] - 1s 4ms/step - loss: 0.0170 - accuracy: 0.9971 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
      "Epoch 2/6\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 0.9999 - val_loss: 0.0100 - val_accuracy: 0.9995\n",
      "Epoch 3/6\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 0.9998 - val_loss: 0.0099 - val_accuracy: 0.9995\n",
      "Epoch 4/6\n",
      "127/127 [==============================] - 0s 3ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 0.9995\n",
      "Epoch 5/6\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9997 - val_loss: 0.0096 - val_accuracy: 0.9995\n",
      "Epoch 6/6\n",
      "127/127 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.0092 - val_accuracy: 1.0000\n",
      "(12, 2) (332, 12)\n",
      "(20, 128) (784, 20)\n",
      "(784, 312)\n",
      "(312,)\n",
      "(312, 128)\n",
      "(128,)\n",
      "(128, 2)\n",
      "(2,)\n",
      "(784, 332)\n",
      "(332,)\n",
      "(332, 140)\n",
      "(140,)\n",
      "(140, 2)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "final_weight = split_expansion(model_t2, model_t1_prev_w, model_new_w, (images_train, labels_train, images_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 0s 919us/step - loss: 0.0122 - accuracy: 1.0000\n",
      "59/59 [==============================] - 0s 912us/step - loss: 0.0164 - accuracy: 0.9979\n"
     ]
    }
   ],
   "source": [
    "new_model_t1, new_model_t2 = make_models_by_weight(weight = final_weight, Task_num = 2, prev_models = [model_t1])\n",
    "\n",
    "_ = new_model_t2.evaluate(images_test,labels_test)\n",
    "#(images_train, labels_train),(images_test, labels_test) = load_mnist()\n",
    "(images_train, labels_train),(images_test, labels_test) = specific_load('mnist', add_channel = False, choose = [4,5])\n",
    "_ = new_model_t1.evaluate(images_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 332)               260620    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 140)               46620     \n",
      "_________________________________________________________________\n",
      "T3_output (Dense)            (None, 2)                 282       \n",
      "=================================================================\n",
      "Total params: 307,522\n",
      "Trainable params: 282\n",
      "Non-trainable params: 307,240\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.7954 - val_loss: 0.4445 - val_accuracy: 0.8208\n",
      "Epoch 2/10\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.4162 - accuracy: 0.8396 - val_loss: 0.4038 - val_accuracy: 0.8355\n",
      "Epoch 3/10\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3876 - accuracy: 0.8503 - val_loss: 0.3740 - val_accuracy: 0.8497\n",
      "Epoch 4/10\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3622 - accuracy: 0.8624 - val_loss: 0.3488 - val_accuracy: 0.8653\n",
      "Epoch 5/10\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3406 - accuracy: 0.8723 - val_loss: 0.3246 - val_accuracy: 0.8746\n",
      "Epoch 6/10\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3221 - accuracy: 0.8812 - val_loss: 0.3076 - val_accuracy: 0.8795\n",
      "Epoch 7/10\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3068 - accuracy: 0.8868 - val_loss: 0.2923 - val_accuracy: 0.8903\n",
      "Epoch 8/10\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.2929 - accuracy: 0.8944 - val_loss: 0.2769 - val_accuracy: 0.8991\n",
      "Epoch 9/10\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.2811 - accuracy: 0.9016 - val_loss: 0.2655 - val_accuracy: 0.9050\n",
      "Epoch 10/10\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.2711 - accuracy: 0.9066 - val_loss: 0.2544 - val_accuracy: 0.9158\n",
      "seleted_params ------\n",
      "\n",
      "layer3/weight (128, 2)\n",
      "layer3/biases (2,)\n",
      "layer2/weight (311, 128)\n",
      "layer2/biases (128,)\n",
      "layer1/weight (784, 311)\n",
      "layer1/biases (311,)\n",
      "Model: \"custom_model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 311)               244135    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               39936     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 284,329\n",
      "Trainable params: 284,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 21.6869 - accuracy: 0.9567 - val_loss: 12.6938 - val_accuracy: 0.9843\n",
      "Epoch 2/10\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 15.6742 - accuracy: 0.9713 - val_loss: 15.1526 - val_accuracy: 0.9745\n",
      "Epoch 3/10\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 15.7663 - accuracy: 0.9713 - val_loss: 12.0878 - val_accuracy: 0.9838\n",
      "Epoch 4/10\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 15.8054 - accuracy: 0.9706 - val_loss: 13.5096 - val_accuracy: 0.9789\n",
      "Epoch 5/10\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 15.3884 - accuracy: 0.9737 - val_loss: 13.5291 - val_accuracy: 0.9775\n",
      "Epoch 6/10\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 16.6653 - accuracy: 0.9694 - val_loss: 14.0734 - val_accuracy: 0.9775\n",
      "Epoch 7/10\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 16.0042 - accuracy: 0.9696 - val_loss: 12.6262 - val_accuracy: 0.9843\n",
      "Epoch 8/10\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 15.5854 - accuracy: 0.9728 - val_loss: 15.8708 - val_accuracy: 0.9785\n",
      "Epoch 9/10\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 16.0608 - accuracy: 0.9725 - val_loss: 13.0358 - val_accuracy: 0.9809\n",
      "Epoch 10/10\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 15.5371 - accuracy: 0.9715 - val_loss: 26.3307 - val_accuracy: 0.9285\n",
      "64/64 [==============================] - 0s 768us/step - loss: 0.7090 - accuracy: 0.7493\n",
      "0.7090471982955933 > 0.01\n",
      "Model: \"custom_model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer (InputLayer)        [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 332)          260620      input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer1_tw (Dense)               (None, 10)           7850        input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer1_tw_concat (Concatenate)  (None, 342)          0           dense_4[0][0]                    \n",
      "                                                                 layer1_tw[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer2_bw (Dense)               (None, 140)          1400        layer1_tw[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 140)          46620       dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer2_tw (Dense)               (None, 10)           3430        layer1_tw_concat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer2_bw_add (Add)             (None, 140)          0           layer2_bw[0][0]                  \n",
      "                                                                 dense_5[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer3_bw (Dense)               (None, 2)            20          layer2_tw[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "T3_output (Dense)               (None, 2)            282         layer2_bw_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer3_add (Add)                (None, 2)            0           layer3_bw[0][0]                  \n",
      "                                                                 T3_output[1][0]                  \n",
      "==================================================================================================\n",
      "Total params: 320,222\n",
      "Trainable params: 12,982\n",
      "Non-trainable params: 307,240\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/6\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.1880 - accuracy: 0.9269 - val_loss: 0.0649 - val_accuracy: 0.9819\n",
      "Epoch 2/6\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.0714 - accuracy: 0.9805 - val_loss: 0.0454 - val_accuracy: 0.9873\n",
      "Epoch 3/6\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.0553 - accuracy: 0.9838 - val_loss: 0.0462 - val_accuracy: 0.9858\n",
      "Epoch 4/6\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.0455 - accuracy: 0.9878 - val_loss: 0.0419 - val_accuracy: 0.9878\n",
      "Epoch 5/6\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.0390 - accuracy: 0.9906 - val_loss: 0.0356 - val_accuracy: 0.9912\n",
      "Epoch 6/6\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.0369 - accuracy: 0.9907 - val_loss: 0.0355 - val_accuracy: 0.9897\n",
      "(10, 2) (342, 10)\n",
      "   [*] Expanding 2th hidden unit, 10 unit added\n",
      "(10, 140) (784, 10)\n",
      "   [*] Expanding 1th hidden unit, 10 unit added\n",
      "(784, 332)\n",
      "(332,)\n",
      "(332, 140)\n",
      "(140,)\n",
      "(140, 2)\n",
      "(2,)\n",
      "(784, 342)\n",
      "(342,)\n",
      "(342, 150)\n",
      "(150,)\n",
      "(150, 2)\n",
      "(2,)\n",
      "   [*] split N in layer: 10 / 58\n",
      "   [*] split N in layer: 10 / 41\n",
      "expanded_w shape = (784, 352)\n",
      "expanded_b shape = (352,)\n",
      "expanded_w shape = (352, 160)\n",
      "expanded_b shape = (160,)\n",
      "[10, 10] [10, 10] [20, 20]\n",
      "Model: \"custom_model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer (InputLayer)        [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 332)          260620      input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer1_tw_select (Dense)        (None, 20)           15700       input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer1_tw_concat_select (Concat (None, 352)          0           dense_4[0][0]                    \n",
      "                                                                 layer1_tw_select[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer2_bw_select (Dense)        (None, 140)          2800        layer1_tw_select[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 140)          46620       dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer2_tw_select (Dense)        (None, 20)           7060        layer1_tw_concat_select[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer2_bw_add_select (Add)      (None, 140)          0           layer2_bw_select[0][0]           \n",
      "                                                                 dense_5[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer3_bw_select (Dense)        (None, 2)            40          layer2_tw_select[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "T3_output (Dense)               (None, 2)            282         layer2_bw_add_select[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer3_add_select (Add)         (None, 2)            0           layer3_bw_select[0][0]           \n",
      "                                                                 T3_output[2][0]                  \n",
      "==================================================================================================\n",
      "Total params: 333,122\n",
      "Trainable params: 25,882\n",
      "Non-trainable params: 307,240\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/6\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.0509 - accuracy: 0.9854 - val_loss: 0.0428 - val_accuracy: 0.9897\n",
      "Epoch 2/6\n",
      "121/121 [==============================] - 1s 5ms/step - loss: 0.0317 - accuracy: 0.9938 - val_loss: 0.0392 - val_accuracy: 0.9892\n",
      "Epoch 3/6\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 0.9952 - val_loss: 0.0341 - val_accuracy: 0.9927\n",
      "Epoch 4/6\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.9947 - val_loss: 0.0289 - val_accuracy: 0.9922\n",
      "Epoch 5/6\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.9964 - val_loss: 0.0305 - val_accuracy: 0.9922\n",
      "Epoch 6/6\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.9978 - val_loss: 0.0326 - val_accuracy: 0.9927\n",
      "(20, 2) (352, 20)\n",
      "(20, 140) (784, 20)\n",
      "(784, 332)\n",
      "(332,)\n",
      "(332, 140)\n",
      "(140,)\n",
      "(140, 2)\n",
      "(2,)\n",
      "(784, 352)\n",
      "(352,)\n",
      "(352, 160)\n",
      "(160,)\n",
      "(160, 2)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "model_t1_prev_w = copy.deepcopy(new_model_t1.get_weights())\n",
    "new_model_t1.trainable = False\n",
    "new_model_t2.trainable = False\n",
    "\n",
    "# cur_w = network_2.trainable_variables\n",
    "# prev_w = list(map(lambda x:x.numpy(), network_2.trainable_variables))\n",
    "model_t3 = new_task_model(tree_model = new_model_t1, Task_number = 3)\n",
    "model_t3.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_t3.summary()\n",
    "\n",
    "#(images_train, labels_train),(images_test, labels_test) = specific_load('emnist', add_channel = False, emnist_type = 'upper', choose = list(range(10)))\n",
    "(images_train, labels_train),(images_test, labels_test) = specific_load('mnist', add_channel = False, choose = [2,3])\n",
    "_ = model_t3.fit(images_train, labels_train, validation_data=(images_test, labels_test), epochs=10, batch_size=100, verbose=1)\n",
    "\n",
    "selective_learning(model_t3, (images_train, labels_train, images_test, labels_test))\n",
    "\n",
    "history = model_t3.evaluate(images_test,labels_test)\n",
    "\n",
    "if history[0] > loss_thr:\n",
    "    ### distory_graph 나중에 추가할 것\n",
    "    print(f\"{history[0]} > {loss_thr}\")\n",
    "    model_new_w = network_expansion(model_t3, ex_k, (images_train, labels_train, images_test, labels_test))\n",
    "else:\n",
    "    model_new_w = model_t3.get_weights\n",
    "\n",
    "final_weight = split_expansion(model_t3, model_t1_prev_w, model_new_w, (images_train, labels_train, images_test, labels_test))\n",
    "\n",
    "new_model_t1, new_model_t2, new_model_t3 = make_models_by_weight(weight = final_weight, Task_num = 3, prev_models = [new_model_t1, new_model_t2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 0s 874us/step - loss: 0.0428 - accuracy: 0.9902\n",
      "67/67 [==============================] - 0s 967us/step - loss: 0.9181 - accuracy: 0.7612\n",
      "59/59 [==============================] - 0s 929us/step - loss: 0.0218 - accuracy: 0.9979\n"
     ]
    }
   ],
   "source": [
    "_ = new_model_t3.evaluate(images_test,labels_test)\n",
    "#(images_train, labels_train),(images_test, labels_test) = load_fashion_mnist()\n",
    "(images_train, labels_train),(images_test, labels_test) = specific_load('mnist', add_channel = False, choose = [0,1])\n",
    "_ = new_model_t2.evaluate(images_test,labels_test)\n",
    "#(images_train, labels_train),(images_test, labels_test) = load_mnist()\n",
    "(images_train, labels_train),(images_test, labels_test) = specific_load('mnist', add_channel = False, choose = [4,5])\n",
    "_ = new_model_t1.evaluate(images_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_2 = model_t2.get_weights()\n",
    "temp_2_1 = new_model_t2.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer : 1\n",
      "169568\n",
      "layer : 2\n",
      "305\n",
      "layer : 3\n",
      "36063\n",
      "layer : 4\n",
      "118\n",
      "layer : 5\n",
      "0\n",
      "layer : 6\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "layer = 1\n",
    "for w1, w2 in zip(temp_2, temp_2_1):\n",
    "    print(\"layer :\", layer)\n",
    "    if len(w1.shape) > 1:\n",
    "        print(np.count_nonzero(w1 != w2[:w1.shape[0], :w1.shape[1]]))\n",
    "    else:\n",
    "        print(np.count_nonzero(w1 != w2[:w1.shape[0]]))\n",
    "    layer = layer + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 352)               276320    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 160)               56480     \n",
      "_________________________________________________________________\n",
      "T4_output (Dense)            (None, 2)                 322       \n",
      "=================================================================\n",
      "Total params: 333,122\n",
      "Trainable params: 322\n",
      "Non-trainable params: 332,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.4897 - accuracy: 0.8348 - val_loss: 0.3234 - val_accuracy: 0.9371\n",
      "Epoch 2/10\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.2768 - accuracy: 0.9405 - val_loss: 0.2342 - val_accuracy: 0.9461\n",
      "Epoch 3/10\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.2147 - accuracy: 0.9515 - val_loss: 0.1959 - val_accuracy: 0.9522\n",
      "Epoch 4/10\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1816 - accuracy: 0.9592 - val_loss: 0.1703 - val_accuracy: 0.9592\n",
      "Epoch 5/10\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9629 - val_loss: 0.1538 - val_accuracy: 0.9653\n",
      "Epoch 6/10\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1439 - accuracy: 0.9670 - val_loss: 0.1416 - val_accuracy: 0.9678\n",
      "Epoch 7/10\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9700 - val_loss: 0.1308 - val_accuracy: 0.9738\n",
      "Epoch 8/10\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9733 - val_loss: 0.1241 - val_accuracy: 0.9708\n",
      "Epoch 9/10\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9741 - val_loss: 0.1172 - val_accuracy: 0.9723\n",
      "Epoch 10/10\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9763 - val_loss: 0.1108 - val_accuracy: 0.9743\n",
      "seleted_params ------\n",
      "\n",
      "layer3/weight (128, 2)\n",
      "layer3/biases (2,)\n",
      "layer2/weight (311, 128)\n",
      "layer2/biases (128,)\n",
      "layer1/weight (784, 311)\n",
      "layer1/biases (311,)\n",
      "Model: \"custom_model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 311)               244135    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 128)               39936     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 284,329\n",
      "Trainable params: 284,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 8.4168 - accuracy: 0.9892 - val_loss: 5.4375 - val_accuracy: 0.9924\n",
      "Epoch 2/10\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 4.8889 - accuracy: 0.9966 - val_loss: 5.0907 - val_accuracy: 0.9935\n",
      "Epoch 3/10\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 5.5850 - accuracy: 0.9947 - val_loss: 5.8854 - val_accuracy: 0.9950\n",
      "Epoch 4/10\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 5.0723 - accuracy: 0.9957 - val_loss: 5.1429 - val_accuracy: 0.9965\n",
      "Epoch 5/10\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 5.3281 - accuracy: 0.9950 - val_loss: 6.1499 - val_accuracy: 0.9935\n",
      "Epoch 6/10\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 5.6520 - accuracy: 0.9947 - val_loss: 5.8884 - val_accuracy: 0.9894\n",
      "Epoch 7/10\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 5.2403 - accuracy: 0.9955 - val_loss: 5.1015 - val_accuracy: 0.9955\n",
      "Epoch 8/10\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 5.1329 - accuracy: 0.9956 - val_loss: 5.9630 - val_accuracy: 0.9899\n",
      "Epoch 9/10\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 4.9812 - accuracy: 0.9955 - val_loss: 5.6649 - val_accuracy: 0.9904\n",
      "Epoch 10/10\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 5.6912 - accuracy: 0.9956 - val_loss: 5.1134 - val_accuracy: 0.9960\n",
      "63/63 [==============================] - 0s 822us/step - loss: 0.1058 - accuracy: 0.9658\n",
      "0.10581885278224945 > 0.01\n",
      "Model: \"custom_model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer (InputLayer)        [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 352)          276320      input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer1_tw (Dense)               (None, 10)           7850        input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer1_tw_concat (Concatenate)  (None, 362)          0           dense_9[0][0]                    \n",
      "                                                                 layer1_tw[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer2_bw (Dense)               (None, 160)          1600        layer1_tw[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 160)          56480       dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer2_tw (Dense)               (None, 10)           3630        layer1_tw_concat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer2_bw_add (Add)             (None, 160)          0           layer2_bw[0][0]                  \n",
      "                                                                 dense_10[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer3_bw (Dense)               (None, 2)            20          layer2_tw[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "T4_output (Dense)               (None, 2)            322         layer2_bw_add[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "layer3_add (Add)                (None, 2)            0           layer3_bw[0][0]                  \n",
      "                                                                 T4_output[1][0]                  \n",
      "==================================================================================================\n",
      "Total params: 346,222\n",
      "Trainable params: 13,422\n",
      "Non-trainable params: 332,800\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/6\n",
      "122/122 [==============================] - 1s 4ms/step - loss: 0.0412 - accuracy: 0.9907 - val_loss: 0.0248 - val_accuracy: 0.9960\n",
      "Epoch 2/6\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9979 - val_loss: 0.0207 - val_accuracy: 0.9990\n",
      "Epoch 3/6\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 0.9991 - val_loss: 0.0200 - val_accuracy: 0.9980\n",
      "Epoch 4/6\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0171 - accuracy: 0.9996 - val_loss: 0.0197 - val_accuracy: 0.9990\n",
      "Epoch 5/6\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 0.9995 - val_loss: 0.0208 - val_accuracy: 0.9980\n",
      "Epoch 6/6\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0162 - accuracy: 0.9997 - val_loss: 0.0188 - val_accuracy: 0.9980\n",
      "(10, 2) (362, 10)\n",
      "   [*] Expanding 2th hidden unit, 10 unit added\n",
      "(10, 160) (784, 10)\n",
      "   [*] Expanding 1th hidden unit, 10 unit added\n",
      "(784, 352)\n",
      "(352,)\n",
      "(352, 160)\n",
      "(160,)\n",
      "(160, 2)\n",
      "(2,)\n",
      "(784, 362)\n",
      "(362,)\n",
      "(362, 170)\n",
      "(170,)\n",
      "(170, 2)\n",
      "(2,)\n",
      "   [*] split N in layer: 10 / 76\n",
      "   [*] split N in layer: 10 / 48\n",
      "expanded_w shape = (784, 372)\n",
      "expanded_b shape = (372,)\n",
      "expanded_w shape = (372, 180)\n",
      "expanded_b shape = (180,)\n",
      "[10, 10] [10, 10] [20, 20]\n",
      "Model: \"custom_model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer (InputLayer)        [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 352)          276320      input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer1_tw_select (Dense)        (None, 20)           15700       input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "layer1_tw_concat_select (Concat (None, 372)          0           dense_9[0][0]                    \n",
      "                                                                 layer1_tw_select[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer2_bw_select (Dense)        (None, 160)          3200        layer1_tw_select[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 160)          56480       dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer2_tw_select (Dense)        (None, 20)           7460        layer1_tw_concat_select[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer2_bw_add_select (Add)      (None, 160)          0           layer2_bw_select[0][0]           \n",
      "                                                                 dense_10[2][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer3_bw_select (Dense)        (None, 2)            40          layer2_tw_select[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "T4_output (Dense)               (None, 2)            322         layer2_bw_add_select[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer3_add_select (Add)         (None, 2)            0           layer3_bw_select[0][0]           \n",
      "                                                                 T4_output[2][0]                  \n",
      "==================================================================================================\n",
      "Total params: 359,522\n",
      "Trainable params: 26,722\n",
      "Non-trainable params: 332,800\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/6\n",
      "122/122 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 0.9998 - val_loss: 0.0276 - val_accuracy: 0.9955\n",
      "Epoch 2/6\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0159 - accuracy: 0.9996 - val_loss: 0.0213 - val_accuracy: 0.9980\n",
      "Epoch 3/6\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 0.9998 - val_loss: 0.0225 - val_accuracy: 0.9985\n",
      "Epoch 4/6\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0226 - val_accuracy: 0.9980\n",
      "Epoch 5/6\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 0.9980\n",
      "Epoch 6/6\n",
      "122/122 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.0221 - val_accuracy: 0.9980\n",
      "(20, 2) (372, 20)\n",
      "(20, 160) (784, 20)\n",
      "(784, 352)\n",
      "(352,)\n",
      "(352, 160)\n",
      "(160,)\n",
      "(160, 2)\n",
      "(2,)\n",
      "(784, 372)\n",
      "(372,)\n",
      "(372, 180)\n",
      "(180,)\n",
      "(180, 2)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "model_t1_prev_w = copy.deepcopy(new_model_t1.get_weights())\n",
    "new_model_t1.trainable = False\n",
    "\n",
    "# cur_w = network_2.trainable_variables\n",
    "# prev_w = list(map(lambda x:x.numpy(), network_2.trainable_variables))\n",
    "model_t4 = new_task_model(tree_model = new_model_t1, Task_number = 4)\n",
    "model_t4.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_t4.summary()\n",
    "\n",
    "#(images_train, labels_train),(images_test, labels_test) = specific_load('emnist', add_channel = False, emnist_type = 'upper', choose = list(range(10)))\n",
    "(images_train, labels_train),(images_test, labels_test) = specific_load('mnist', add_channel = False, choose = [6,7])\n",
    "_ = model_t4.fit(images_train, labels_train, validation_data=(images_test, labels_test), epochs=10, batch_size=100, verbose=1)\n",
    "\n",
    "selective_learning(model_t4, (images_train, labels_train, images_test, labels_test))\n",
    "\n",
    "history = model_t4.evaluate(images_test,labels_test)\n",
    "\n",
    "if history[0] > loss_thr:\n",
    "    ### distory_graph 나중에 추가할 것\n",
    "    print(f\"{history[0]} > {loss_thr}\")\n",
    "    model_new_w = network_expansion(model_t4, ex_k, (images_train, labels_train, images_test, labels_test))\n",
    "else:\n",
    "    model_new_w = model_t3.get_weights\n",
    "\n",
    "final_weight = split_expansion(model_t4, model_t1_prev_w, model_new_w, (images_train, labels_train, images_test, labels_test))\n",
    "\n",
    "new_model_t1, new_model_t2, new_model_t3, new_model_t4 = make_models_by_weight(weight = final_weight, Task_num = 4, prev_models = [new_model_t1, new_model_t2, new_model_t3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 897us/step - loss: 0.0250 - accuracy: 0.9985\n",
      "64/64 [==============================] - 0s 946us/step - loss: 0.7799 - accuracy: 0.8575\n",
      "67/67 [==============================] - 0s 939us/step - loss: 0.8845 - accuracy: 0.8331\n",
      "59/59 [==============================] - 0s 944us/step - loss: 0.0198 - accuracy: 0.9989\n"
     ]
    }
   ],
   "source": [
    "_ = new_model_t4.evaluate(images_test,labels_test)\n",
    "(images_train, labels_train),(images_test, labels_test) = specific_load('mnist', add_channel = False, choose = [2,3])\n",
    "_ = new_model_t3.evaluate(images_test,labels_test)\n",
    "#(images_train, labels_train),(images_test, labels_test) = load_fashion_mnist()\n",
    "(images_train, labels_train),(images_test, labels_test) = specific_load('mnist', add_channel = False, choose = [0,1])\n",
    "_ = new_model_t2.evaluate(images_test,labels_test)\n",
    "#(images_train, labels_train),(images_test, labels_test) = load_mnist()\n",
    "(images_train, labels_train),(images_test, labels_test) = specific_load('mnist', add_channel = False, choose = [4,5])\n",
    "_ = new_model_t1.evaluate(images_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t1_prev_w = copy.deepcopy(new_model_t1.get_weights())\n",
    "new_model_t1.trainable = False\n",
    "\n",
    "# cur_w = network_2.trainable_variables\n",
    "# prev_w = list(map(lambda x:x.numpy(), network_2.trainable_variables))\n",
    "model_t5 = new_task_model(tree_model = new_model_t1, Task_number = 4)\n",
    "model_t5.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model_t5.summary()\n",
    "\n",
    "#(images_train, labels_train),(images_test, labels_test) = specific_load('emnist', add_channel = False, emnist_type = 'upper', choose = list(range(10)))\n",
    "(images_train, labels_train),(images_test, labels_test) = specific_load('mnist', add_channel = False, choose = [8,9])\n",
    "_ = model_t5.fit(images_train, labels_train, validation_data=(images_test, labels_test), epochs=10, batch_size=100, verbose=1)\n",
    "\n",
    "selective_learning(model_t5, (images_train, labels_train, images_test, labels_test))\n",
    "\n",
    "history = model_t5.evaluate(images_test,labels_test)\n",
    "\n",
    "if history[0] > loss_thr:\n",
    "    ### distory_graph 나중에 추가할 것\n",
    "    print(f\"{history[0]} > {loss_thr}\")\n",
    "    model_new_w = network_expansion(model_t5, ex_k, (images_train, labels_train, images_test, labels_test))\n",
    "else:\n",
    "    model_new_w = model_t3.get_weights\n",
    "\n",
    "final_weight = split_expansion(model_t5, model_t1_prev_w, model_new_w, (images_train, labels_train, images_test, labels_test))\n",
    "\n",
    "new_model_t1, new_model_t2, new_model_t3, new_model_t4, new_model_t5 = make_models_by_weight(weight = final_weight, Task_num = 4, prev_models = [new_model_t1, new_model_t2, new_model_t3, new_model_t4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "1ec7e7a7bda2752b69488ef6b463cd212a85bea9beda62a0e3d72f3155be411c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
